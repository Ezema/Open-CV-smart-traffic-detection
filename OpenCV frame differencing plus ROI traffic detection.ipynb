{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd0fb8a8",
   "metadata": {},
   "source": [
    "# OpenCV road traffic detection plus region of interest counting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80e755",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The moving cars are successfully detected using the frame differencing and background subtraction techniques.\n",
    "To detect when those detected cars go to the city centre, a rectangle-shaped [Region of interest](https://en.wikipedia.org/wiki/Region_of_interest) was created (visually represented by coloring it with orange). When the cars pass through the region of interest, the counter gets updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install openCV\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57c17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Create the function that will be used for both videos\n",
    "def calculateCityCentreTrafficFromVideoPathAndShowVideo(videoPath):\n",
    "    #Create a CV2 video capture object from videoPath\n",
    "    videoCaptureObject = cv2.VideoCapture(videoPath) \n",
    "\n",
    "    # Exit if the video could not be opened\n",
    "    if not videoCaptureObject.isOpened(): \n",
    "        print(\"An error has occurred while trying to open the video\")\n",
    "        exit(1)\n",
    "\n",
    "    # Get the video playback length\n",
    "    framesPerSeconds = videoCaptureObject.get(cv2.CAP_PROP_FPS) \n",
    "    totalNumberOfFrames = int(videoCaptureObject.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    videoPlaybackLengthInSeconds = totalNumberOfFrames/framesPerSeconds\n",
    "        \n",
    "    # Instantiate a KNN background substractor https://docs.opencv.org/3.4/de/de1/group__video__motion.html\n",
    "    backgroundSubstractorObjectInstance = cv2.createBackgroundSubtractorKNN(detectShadows = False)\n",
    "\n",
    "    # This global variable will keep track of the number of cars that go the city centre    \n",
    "    global carCounter\n",
    "    carCounter = 0\n",
    "\n",
    "    # This are the region of interest position and dimensions\n",
    "    # Their definition in this section of the notebook makes it easier to change its values if so wanted\n",
    "    # **IMPORTANT**: making the ROI width or height small may result in the counter algorithm\n",
    "    # fail to detect that the car's contour middlepoint is inside of it when analysing each of the video frames.\n",
    "    # Similarly, making the ROI's width too long (bigger that the usual distance that is kept between cars when driving) \n",
    "    # may make the counting give false readings when two cars are on the same street one right next to the other.\n",
    "    # A detector's width of 20 and a height of 100 had been tested to work correctly.\n",
    "    # Obviously, the position and height of the ROI is crucial in avoiding to detect the other side of the road cars.\n",
    "    detectorOverlayX,detectorOverlayY,detectorOverlayWidth,detectorOverlayHeight = 340, 340, 20, 100\n",
    "\n",
    "    # This global boolean variable is used as a flag to avoid counting more than once when the same car is\n",
    "    # going through inside the ROI in multiple frames. \n",
    "    global carAlreadyDetected\n",
    "    carAlreadyDetected = False\n",
    "\n",
    "    # Initiate a loop that will iterate over every frame sequentially\n",
    "    while True:    \n",
    "\n",
    "        # Read the video frame\n",
    "        wasFrameReaded, videoFrame = videoCaptureObject.read()\n",
    "\n",
    "        # If the frame had been successfully read continue, otherwise break the loop and stop the application\n",
    "        if wasFrameReaded == True:        \n",
    "\n",
    "            # For each of the frames of the video the first frame of the video (that is considered the background) will \n",
    "            # be substracted to obtain the difference between the two: the delta frame\n",
    "            deltaFrame= backgroundSubstractorObjectInstance.apply(videoFrame)\n",
    "            \n",
    "            # Erosion: Removes pixels from the edges of objects in an image.        \n",
    "            deltaFrame = cv2.erode(deltaFrame, None,50)        \n",
    "\n",
    "            #The findContours function will retrieve the boundaries of the objects in the deltaFrame image\n",
    "            contours,_=cv2.findContours(deltaFrame,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # Create an reactangle overlay to show in the video the section that gets ignored \n",
    "            # Overlay coude source: https://gist.github.com/IAmSuyogJadhav/305bfd9a0605a4c096383408bee7fd5c\n",
    "            overlayRectangleX, overlayRectangleY, overlayRectangleWidth, overlayRectangleHeight = 3, 3, 1032, 259\n",
    "            overlay = videoFrame.copy()\n",
    "            cv2.rectangle(overlay, (overlayRectangleX, overlayRectangleY), (overlayRectangleX+overlayRectangleWidth, overlayRectangleY+overlayRectangleHeight), (0, 0, 0), -1)\n",
    "            alpha = 0.6\n",
    "            videoFrame = cv2.addWeighted(overlay, alpha, videoFrame, 1 - alpha, 0)\n",
    "\n",
    "            # Create a transparent rectangle to visually represent the area that detects the cars going to the\n",
    "            # city centre\n",
    "            overlay = videoFrame.copy()\n",
    "            cv2.rectangle(overlay, (detectorOverlayX, detectorOverlayY), (detectorOverlayX+detectorOverlayWidth, detectorOverlayY+detectorOverlayHeight), (0, 165, 255), -1)\n",
    "            alpha = 0.6\n",
    "            videoFrame = cv2.addWeighted(overlay, alpha, videoFrame, 1 - alpha, 0)\n",
    "\n",
    "            # Create the 'Main Street' text and the rectangle that sourrounds it\n",
    "            cv2.putText(videoFrame, 'Main Street', (5, 250), cv2.FONT_HERSHEY_TRIPLEX, 1, (0,0,255))\n",
    "            cv2.line(videoFrame,(3,262), (1035,262), (0,0,255),3)\n",
    "            cv2.line(videoFrame,(3,262), (3,595), (0,0,255),3)\n",
    "            cv2.line(videoFrame,(1035,262), (1035,595), (0,0,255),3)\n",
    "            cv2.line(videoFrame,(3,595), (1035,595), (0,0,255),3)\n",
    "\n",
    "            # Create the 'City centre' text and the arrow that points in that direction            \n",
    "            cv2.putText(videoFrame, 'City Centre', (30, 400), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 165, 255))        \n",
    "            videoFrame = cv2.arrowedLine(videoFrame,(200,370),(30,360),(0,165,255), 2)               \n",
    "\n",
    "            # Iterate over the contours previously obtained.\n",
    "            for contour in contours:\n",
    "                # If the area of the contour is less than 3000 skip the loop (do nothing)\n",
    "                # The 3000 number was obtained from testing with the video which number was the\n",
    "                # minimum to avoid marking the people walking on the main street as well as the bicycles\n",
    "                if cv2.contourArea(contour) < 3000:\n",
    "                    continue        \n",
    "                # The boundingRect function calculates the tightest rectangle that encloses the contour object\n",
    "                (x, y, width, height)=cv2.boundingRect(contour)\n",
    "                # If the position of the contour's enclosing rectangle is outside the main street proceed to ignore it\n",
    "                if y<262:\n",
    "                    continue\n",
    "\n",
    "                # Calculate the midpoint (x and y) of the contour's bounding rectangle\n",
    "                xMiddleOfContour = int((x + (x+width)) / 2) \n",
    "                yMiddleOfContour = int((y + (y+height)) / 2)            \n",
    "\n",
    "                # This conditional will filter out the contours's middlepoint that are not inside the vertical area delimited\n",
    "                # by the position where the ROI region starts (y) and where it ends (y+height)\n",
    "                if yMiddleOfContour>=detectorOverlayY and yMiddleOfContour<=detectorOverlayY+detectorOverlayHeight:                            \n",
    "                    # This conditional will allow the contours's middlepoint that are inside the ROI's area \n",
    "                    # (delimited by its x and x+width). Additionally, will check the flag carAlreadyDetected.\n",
    "                    # If the car contour's middlepoint is inside the ROI and the flag carAlreadyDetected is false, the\n",
    "                    # counter will be updated and the flag will be set to true. \n",
    "                    # The carAlreadyDetected flag will be resetted (to allow the counting of the next car that\n",
    "                    # goes through the ROI) when the car leaves the ROI.\n",
    "                    # Technically, \"leaving the ROI\" is delimited by a (non-visually represented) area that has the same width as the ROI                    \n",
    "                    # but is immediately at its left. This will reset the ROI and the next car will be detected.\n",
    "                    if xMiddleOfContour>=detectorOverlayX and xMiddleOfContour<=detectorOverlayX+detectorOverlayWidth and not carAlreadyDetected:                    \n",
    "                        carAlreadyDetected=True\n",
    "                        carCounter=carCounter+1                                        \n",
    "                    elif (xMiddleOfContour+width/2)<detectorOverlayX and (xMiddleOfContour+width/2)>(detectorOverlayX-detectorOverlayWidth):                    \n",
    "                        carAlreadyDetected = False\n",
    "\n",
    "                    # Only for aesthetic purposes, show the contour rectangle and the middle point when \n",
    "                    # the car is close to the overlay counter region, these lines can be removed if so wanted\n",
    "                    if (xMiddleOfContour<=detectorOverlayX and xMiddleOfContour>=detectorOverlayX-detectorOverlayWidth) or (xMiddleOfContour>=detectorOverlayX and xMiddleOfContour<=detectorOverlayX+(detectorOverlayWidth*2)):\n",
    "                        cv2.circle(videoFrame, (xMiddleOfContour, yMiddleOfContour), 1, (255,255,255), 2)\n",
    "                        # The contour's enclosing rectangle is inside the area of the main street            \n",
    "                        cv2.rectangle(videoFrame, (x, y), (x+width, y+height), (255,255,255), 3)\n",
    "\n",
    "            # Show the dynamic text for car counting\n",
    "            cv2.putText(videoFrame, 'Counted cars to city centre: {carCounter}'.format(carCounter=carCounter), (detectorOverlayX, 250), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 165, 255))\n",
    "\n",
    "            # Name the playback window and set its dimensions.\n",
    "            cv2.namedWindow(\"Output video\", cv2.WINDOW_NORMAL)\n",
    "            cv2.resizeWindow(\"Output video\", 1920, 1080)         \n",
    "\n",
    "            # Show the output video that detects the moving cars\n",
    "            cv2.imshow('Output video', videoFrame)\n",
    "\n",
    "            # Press 'q' to 'quit'\n",
    "            if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            # As mentioned above, this will get executed when the frame could not be read\n",
    "            break    \n",
    "    # When the video playback came to an end or the user pressed 'q', proceed to release the videoCapture object and close the window\n",
    "    videoCaptureObject.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    # Print the results to STDOUT\n",
    "    print(\"\\nResult:\")\n",
    "    print('For the video {videoPath} the total number of cars going to the city centre are {carCounter}'.format(videoPath=videoPath, carCounter=carCounter))\n",
    "    \n",
    "    # Calculate the number of cars that go to the city center per minute based on the length of the video\n",
    "    numberOfSecondsInAMinute = 60\n",
    "    carsPerMinute = carCounter/(videoPlaybackLengthInSeconds/numberOfSecondsInAMinute)\n",
    "    return [carCounter,carsPerMinute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf1f06ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "For the video Traffic_Sample_1.mp4 the total number of cars going to the city centre are 6\n",
      "\n",
      "Result:\n",
      "For the video Traffic_Sample_2.mp4 the total number of cars going to the city centre are 4\n"
     ]
    }
   ],
   "source": [
    "videoPathsList = ['Traffic_Sample_1.mp4','Traffic_Sample_2.mp4']\n",
    "carCountResultsList = []\n",
    "for videoPath in videoPathsList:\n",
    "    carCountResultsList.append(calculateCityCentreTrafficFromVideoPathAndShowVideo(videoPath))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e32fa3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total number of cars</th>\n",
       "      <th>Cars per minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Traffic_Laramie_1.mp4</th>\n",
       "      <td>6</td>\n",
       "      <td>2.023381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Traffic_Laramie_2.mp4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.271007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "# Create a pandas dataframe based on the results\n",
    "resultDataframe = pandas.DataFrame(carCountResultsList, columns = ['Total number of cars', 'Cars per minute'], index=videoPathsList)\n",
    "\n",
    "# Showing the table of results in HTML\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(resultDataframe.to_html()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
